[
{"rep_name": "gsoc2014-integration-tests", "description": "GSoC2014 - Scrapy Integration tests project", "url": null, "stars": null, "forks": null, "watching": null, "commits": null, "info_commit": [null, null, null], "releases": null, "info_releases": ["None", null, ""]},
{"rep_name": "slybot", "description": "None", "url": null, "stars": null, "forks": null, "watching": null, "commits": null, "info_commit": [null, null, null], "releases": null, "info_releases": ["None", null, ""]},
{"rep_name": "pypydispatcher", "description": "A fork of", "url": null, "stars": null, "forks": null, "watching": null, "commits": null, "info_commit": [null, null, null], "releases": null, "info_releases": ["None", null, ""]},
{"rep_name": "scrapy-bench-speedcenter", "description": "Codespeed for scrapy-bench", "url": null, "stars": null, "forks": null, "watching": null, "commits": null, "info_commit": [null, null, null], "releases": null, "info_releases": ["None", null, ""]},
{"rep_name": "dirbot", "description": "Scrapy project to scrape public web directories (educational) [DEPRECATED]", "url": null, "stars": null, "forks": null, "watching": null, "commits": null, "info_commit": [null, null, null], "releases": null, "info_releases": ["None", null, ""]},
{"rep_name": "base-chromium", "description": "base component forked from Chromium source", "url": null, "stars": null, "forks": null, "watching": null, "commits": null, "info_commit": [null, null, null], "releases": null, "info_releases": ["None", null, ""]},
{"rep_name": "url-chromium", "description": "url component from Chromium source code, forked from", "url": null, "stars": "2", "forks": null, "watching": null, "commits": null, "info_commit": [null, null, null], "releases": null, "info_releases": ["None", null, ""]},
{"rep_name": "scurl", "description": "Performance-focused replacement for Python urllib", "url": null, "stars": "6", "forks": null, "watching": null, "commits": null, "info_commit": [null, null, null], "releases": null, "info_releases": ["None", null, ""]},
{"rep_name": "scrapely", "description": "A pure-python HTML screen-scraping library", "url": null, "stars": null, "forks": null, "watching": null, "commits": "205", "info_commit": ["ruairif", "Merge pull request", "2019-11-28T10:27:42Z"], "releases": null, "info_releases": ["None", null, ""]},
{"rep_name": "scrapy-itemloader", "description": "[Archived] Library to populate Scrapy items using XPath and CSS with a convenient API", "url": "https://github.com/scrapy/itemloaders", "stars": "7", "forks": null, "watching": null, "commits": null, "info_commit": [null, null, null], "releases": null, "info_releases": ["None", null, ""]},
{"rep_name": "quotesbot", "description": "This is a sample Scrapy project for educational purposes", "url": "http://doc.scrapy.org/en/latest/intro/tutorial.html", "stars": "708", "forks": null, "watching": null, "commits": "5", "info_commit": [null, "point to spidyquotes repo in readme", "2016-10-18T11:16:30Z"], "releases": null, "info_releases": ["None", null, ""]},
{"rep_name": "booksbot", "description": "A crawler for", "url": null, "stars": null, "forks": null, "watching": null, "commits": null, "info_commit": [null, null, null], "releases": null, "info_releases": ["None", null, ""]},
{"rep_name": "loginform", "description": "Fill HTML login forms automatically", "url": null, "stars": null, "forks": null, "watching": null, "commits": null, "info_commit": [null, null, null], "releases": null, "info_releases": ["None", null, ""]},
{"rep_name": "scrapy-bench", "description": "A CLI for benchmarking Scrapy.", "url": null, "stars": "15", "forks": null, "watching": null, "commits": null, "info_commit": [null, null, null], "releases": null, "info_releases": ["None", null, ""]},
{"rep_name": "itemloaders", "description": "Library to populate items using XPath and CSS with a convenient API", "url": null, "stars": null, "forks": null, "watching": null, "commits": null, "info_commit": [null, null, null], "releases": null, "info_releases": ["None", null, ""]},
{"rep_name": "xtractmime", "description": "", "url": null, "stars": null, "forks": null, "watching": null, "commits": null, "info_commit": [null, null, null], "releases": null, "info_releases": ["None", null, ""]},
{"rep_name": "queuelib", "description": "Collection of persistent (disk-based) and non-persistent (memory-based) queues for Python", "url": null, "stars": "49", "forks": null, "watching": null, "commits": "115", "info_commit": ["elacuesta", "Bump version: 1.6.1 â†’ 1.6.2", "2021-08-26T13:39:03Z"], "releases": "3", "info_releases": ["v1.6.2", "2021-08-26T13:41:40Z", "Code quality release, no changes in functionality.  Highlights:   Added  python_requires>=3.5  to  setup.py  Formatted the codebase with  black  Added type annotations  Added CI checks for typing, security and linting "]},
{"rep_name": "cssselect", "description": "CSS Selectors for Python", "url": "https://cssselect.readthedocs.io/", "stars": "22", "forks": null, "watching": "54", "commits": null, "info_commit": [null, null, null], "releases": null, "info_releases": ["None", null, ""]},
{"rep_name": "itemadapter", "description": "Common interface for data container classes", "url": null, "stars": "6", "forks": null, "watching": null, "commits": null, "info_commit": [null, null, null], "releases": "3", "info_releases": ["v0.4.0", "2021-08-26T21:16:47Z", "Added  ItemAdapter.is_item_class  and  ItemAdapter.get_field_meta_from_class  class methods ( #54 )"]},
{"rep_name": "w3lib", "description": "Python library of web-related functions", "url": null, "stars": "91", "forks": null, "watching": null, "commits": null, "info_commit": [null, null, null], "releases": "6", "info_releases": ["v1.22.0", "2020-05-13T19:36:00Z", " Python 3.4 is no longer supported (issue  #156 )  w3lib.url.safe_url_string  now supports an optional  quote_path parameter to disable the percent-encoding of the URL path (issue  #119 )  w3lib.url.add_or_replace_parameter  and  w3lib.url.add_or_replace_parameters  no longer remove duplicate parameters from the original query string that are not being added or replaced (issue  #126 )  w3lib.html.remove_tags  now raises a  ValueError  exception instead of  AssertionError  when using both the  which_ones  and the  keep  parameters (issue  #154 )  Test improvements (issues  #143 ,  #146 ,  #148 ,  #149 )  Documentation improvements (issues  #140 ,  #144 ,  #145 ,  #151 ,  #152 ,  #153 )  Code cleanup (issue  #139 ) "]},
{"rep_name": "scrapyd-client", "description": "Command line client for Scrapyd server", "url": null, "stars": null, "forks": null, "watching": null, "commits": null, "info_commit": [null, null, null], "releases": null, "info_releases": ["None", null, ""]},
{"rep_name": "scrapy.org", "description": "The scrapy.org website", "url": "https://scrapy.org", "stars": "150", "forks": null, "watching": null, "commits": null, "info_commit": [null, null, null], "releases": null, "info_releases": ["None", null, ""]},
{"rep_name": "parsel", "description": "Parsel lets you extract data from XML/HTML documents using XPath or CSS selectors", "url": null, "stars": "114", "forks": null, "watching": null, "commits": "577", "info_commit": ["wRAR", "Add a fix for new Sybil (", "2021-12-22T20:16:39Z"], "releases": "12", "info_releases": ["1.6.0", "2020-05-07T21:28:33Z", " Python 3.4 is no longer supported  New  Selector.remove()  and  SelectorList.remove()  methods to remove selected elements from the parsed document tree  Improvements to error reporting, test coverage and documentation, and code cleanup "]},
{"rep_name": "protego", "description": "A pure-Python robots.txt parser with support for modern conventions.", "url": null, "stars": "18", "forks": null, "watching": null, "commits": null, "info_commit": [null, null, null], "releases": "8", "info_releases": ["Protego 0.2.1", null, " Fixes incorrect readme  content-type  specified in  setup.py  ( #21 ) "]},
{"rep_name": "scrapyd", "description": "A service daemon to run Scrapy spiders", "url": null, "stars": null, "forks": null, "watching": null, "commits": "452", "info_commit": ["pawelmhm", "Merge pull request", "2022-02-24T06:51:01Z"], "releases": "6", "info_releases": ["1.3.0", "2022-01-12T12:58:35Z", "Added   support for HTTP authentication in scrapyd server  Jobs website shortcut to cancel a job using the cancel.json webservice.  Make project argument to listjobs.json optional, so that we can easily query for all jobs.  Python 3.7, 3.8, 3.9, 3.10 support  Configuration option for job storage class  Configuration option for egg storage class  improved HTTP headers in webservice  improved test coverage   Removed   Python 2 support  Python 3.3 support (although never officially supported)  Python 3.4 support  Python 3.5 support  Pypy 2 support  Doc for ubuntu installs, Zyte no longer maintains ubuntu repo.   Fixed   ScrapyD now respects Scrapy TWISTED_REACTOR setting  replaced deprecated SafeConfigParser with ConfigParser "]},
{"rep_name": "scrapy", "description": "Scrapy, a fast high-level web crawling & scraping framework for Python.", "url": "https://scrapy.org", "stars": "42.9k", "forks": "9.5k", "watching": "1.8k", "commits": "9,103", "info_commit": ["wRAR", "Pin old markupsafe when we pin old mitmproxy (", "2022-02-23T18:52:18Z"], "releases": "22", "info_releases": ["2.5.1", "2021-10-05T13:50:01Z", "Security bug fix:  If you use  HttpAuthMiddleware  (i.e. the  http_user  and  http_pass  spider attributes) for HTTP authentication, any request exposes your credentials to the request target.  To prevent unintended exposure of authentication credentials to unintended domains, you must now additionally set a new, additional spider attribute,  http_auth_domain , and point it to the specific domain to which the authentication credentials must be sent.  If the  http_auth_domain  spider attribute is not set, the domain of the first request will be considered the HTTP authentication target, and authentication credentials will only be sent in requests targeting that domain.  If you need to send the same HTTP authentication credentials to multiple domains, you can use  w3lib.http.basic_auth_header  instead to set the value of the  Authorization  header of your requests.  If you  really  want your spider to send the same HTTP authentication credentials to any domain, set the  http_auth_domain  spider attribute to  None .  Finally, if you are a user of  scrapy-splash , know that this version of Scrapy breaks compatibility with scrapy-splash 0.7.2 and earlier. You will need to upgrade scrapy-splash to a greater version for it to continue to work."]}
]